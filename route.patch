diff --git a/app/api/llm/route.ts b/app/api/llm/route.ts 
index 815dfc6..newhash 100644 
--- a/app/api/llm/route.ts 
+++ b/app/api/llm/route.ts 
@@ -1,30 +1,12 @@ 
 import { GoogleGenerativeAI } from "@google/generative-ai";
 import OpenAI from "openai";

-export const maxDuration = 60; 
+export const maxDuration = 300; // allow longer responses for local models

-const generator = new GoogleGenerativeAI(process.env.GEMINI_KEY as string); 
-
-const openai = new OpenAI({ 
-  apiKey: process.env.OPENAI_KEY, 
-}); 
- 
-export async function POST(request: Request) { 
-  const data = await request.json(); 
-  const { prompt, model, key, openrouter } = data; 
-
-  if (openrouter || key) { 
-    // OpenRouter path (removed) 
-    return new Response("OpenRouter support removed", { status: 410 }); 
-  }
-
-  // Keep only generic proxy or remove entirely 
-  // For now, just forward to whatever backend is configured via client-side 
-  return new Response("Use client-side LLM calls", { status: 200 }); 
+// This route can be deleted or kept as fallback
+// Recommendation: delete app/api/llm/route.ts entirely if client-side calls are used
} 
